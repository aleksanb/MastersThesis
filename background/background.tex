\chapter{Background}
\label{chapter:background}

\section{A Brief Introduction to Reservoir Computing}
\label{section:reservoir-computing-introduction}

Recurrent Neural Networks (RNNs), as opposed to feed-forward neural networks,
are notoriously time consuming and difficult to train \cite{Schrauwen2007}.
This is due to feedback from the recurrent connections during the training process,
allowing small topology changes to drastically change a network's position in the fitness landscape.

It was therefore proposed in both \cite{jaeger2002adaptive} (as Echo State Networks, or ESN)
and \cite{natschlager2002liquid} (as Liquid State Machines, or LSM) to separate the RNN into two parts,
the untrained reccurrent reservoir, and the trained readout layer.
Both of these methods have been unified into the field of Reservoir Computing,
now focusing on the separate training and evolution of the recurrent and readout parts \cite{lukovsevivcius2012reservoir}.
Exiting applications of Reservoir Computing include speech and handwriting recognition,
as well as controlling robotics \cite{lukovsevivcius2012reservoir}.

\begin{figure}[ht]
    \caption{
        Schematic of a general Reservoir Computing system containing adjustable biases, feedback loops, reservoir and readout layers,
        which are described in section \ref{section:reservoir-computing-introduction}.
        Inspired by figures from \cite{Schrauwen2007} and \cite{Jaeger:2007}.
    }
    \label{fig:rc-schema}
    \includegraphics[width=\columnwidth]{background/reservoir_computing_schema.pdf}
\end{figure}

A generalized structure of a Reservoir Computing system with all bells and whistles is shown in figure \ref{fig:rc-schema}.
The main components are the reservoir and the readout layer.
At each timestep in the core model,
the reservoir receives the current input signal as well as its previous state (it is recurrent after all).
It transforms it, and passes it on to the readout layer.
The output layer frequently receives the teacher input as well.
The internal weights of the reservoir are usually randomized and left untrained,
with the weights of the readout layer being adjusted by some learning algorithm.
This can be linear or ridge regression for offline learning,
or recursive least sqares for online learning \cite{Schrauwen2007}.

There are many extensions to the base model.
The reservoir and output layers can receive biases:
The readout bias is used for regularizing the reservoir state in case the problem is ill-posed,
but isn't needed when using a model like ridge regression, which performs regularization internally \cite{Schrauwen2007}.
Reservoir bias is used for stabilizing models which feeds the output layer back into the reservoir,
which may be needed if the problem entails producing oscillating output \cite{Jaeger:2007}.
Teacher forcing,
that is forcing the states of the output layer to those expected by the trainer for the first n timesteps,
usually speed up the convergence of the learning method used,
and may in some cases be required to at all achieve stability \cite{jaeger2002tutorial}.

For a deeper dive into Reservoir Computing,
consult papers \cite{Schrauwen2007}, \cite{lukovsevivcius2012reservoir}, and \cite{Jaeger:2007}.

\section{Alternatives to Classical Reservoirs}

Are there other types of complex systems that can be used as reservoirs?
What properties must these reservoirs have to be able to solve problems?

Complex networks similar to the sparsely connected RNNs used  ESN and LSM systems include Cellular Automata \cite{wolfram2002new} and Random Boolean Networks \cite{kauffman1969metabolic}.
Cellular Automata are regular grids of cells containing some state,
each cell connected to its neighbors in the grid.
Cells then update in lockstep according to some shared transition table,
creating a new generation.
RBNs can be seen upon as an abstraction over CAs again,
allowing for nonlocal neighbors and variable updating rules.
They will be introduced in depth in section \ref{section:rbns}.

Both CAs and RBNs have successfully been used in Reservoir Computing systems \cite{yilmaz2014reservoir} \cite{rbn-reservoir}.
Both models are simple, and can be implemented in software,
hardware (FPGAs), and in materio \cite{miller2002evolution} \cite{farstad2015evolving}.

This computational paradigm is known as Cellular Computing,
and provides a potentially powerful alternative to classical computers,
leveraging extreme parallelism, simple components and local state \cite{sipper1999emergence}.

\section{A Brief Introduction to Boolean Networks}
\label{section:rbns}

Random Boolean Networks, also known as Kaffman networks,
were originally developed as a model of gene regulatory networks \cite{kauffman1969metabolic},
the complex system that regulates how genes in multicellular organisms interact with each other.
The model requires no assumptions about the inner workings of the actual nodes,
which allows it to model phenomena where the exact internal workings of the system may be unknown.

The simplification of a system to a boolean model doesn't pose a problem,
as any multi-valued network can be transformed to a corresponding binary one.

A RBN is usually described by its number of nodes $N$ and the in-degree $K$ of the nodes,
that is, how many nodes each node depends on (also known as its ancestors).
RBNs can have both homogenous and heterogenous in-degrees.
In heterogenous networks, one usually describes the average connectivity $\langle K \rangle$ instead.

Each node can have a state of zero or one.
The next state of the node is solely determined by the current combination of states of its ancestors.
Each combination leads to a new state of zero or one,
with the probability given by a binomial distribution usually having $\langle P \rangle = 0.5$.
Figure \ref{figure:sample-homogenous-rbn} visualizes a homogenous RBN with $N=3, K=2, P=0.5$.

\begin{figure}
  \centering
  \subfloat[RBN topology]{
    \begin{tikzpicture}[node distance = 5em]
      \node[vertex] (a) {a};
      \node[vertex] (b) [below left of=a] {b};
      \node[vertex] (c) [below right of=a] {c};

      \draw[edge] (a) to[bend right] (b);
      \draw[edge] (a) to (c);

      \draw[edge] (b) to (a);
      \draw[edge] (b) to[bend right] (c);

      \draw[edge] (c) to[bend right] (a);
      \draw[edge] (c) to (b);
    \end{tikzpicture}
  }
  \subfloat[Transition table for node a]{
    \begin{tabular}[b]{ c c | c}
      \multicolumn{2}{c}{Ancestor states} & New state \\
      \hline
      0 & 0 & 0 \\
      0 & 1 & 1 \\
      1 & 0 & 0 \\
      1 & 1 & 1 \\
    \end{tabular}
  }
  \caption{An example homogenous RBN with $N=3, K=2, P=0.5$.}
  \label{figure:sample-homogenous-rbn}
\end{figure}

In the simplest RBN updating scheme, all nodes update in lockstep.
This is known as the Classical RBN updating scheme (CRBN).
The states of the RBN at the next timestep $t+1$ therefore only depend on the states at the previous timestep $t$.
As the number of RBN states is finite ($2^{n\_nodes}$),
the system will eventually revisit a previously visited state.
This set of repeating states is known as an \emph{attractor},
and a deterministic system cannot escape from it.
If the attractor consists of a single state it is known as a point attractor,
otherwise a cycle attractor.
The set of states leading towards an attractor is known as its \emph{basin of attraction}.
A cycle attractor can be observed in figure \ref{figure:rbn-critical},
while a point attractor is observed in figure \ref{figure:rbn-ordered}.

A criticism of the classical model is that gene regulation networks are updating continiously,
as opposed to in lockstep \cite{gershenson2004introduction}.
There are therefore a number of alternate updating schemes which can be categorized by whether they are deterministic or nondeterministic, as well as synchronous and asynchronous.

The dynamics of an RBN can be categorized as being in either the ordered, critical, or chaotic phase.
These phases can be identified by how large a part of the network state is able to change over time,
whether similar states tend to converge or diverge over time,
and the networks resistance to perturbations (outside changes to the network).

One way to obtain these phases analytically is by comparing the resulting states of two identical RBNs where one is subject to some perturbation \cite{gershenson2004introduction}.
For visual identification, we plot the states of the RBN in a square lattice,
with the network states plotted horizontally, and time flowing downwards.
A node is drawn as white if its state is one, black otherwise.
The phases are visualized in Figure \ref{figure:rbn-phases}.

\begin{figure}
  \subfloat[Ordered phase, K=1]{
    \includegraphics[width=0.3\columnwidth]{background/ordered-phase.pdf}
    \label{figure:rbn-ordered}
  }
  \subfloat[Critical phase, K=2]{
    \includegraphics[width=0.3\columnwidth]{background/critical-phase.pdf}
    \label{figure:rbn-critical}
  }
  \subfloat[Chaotic phase, K=3]{
    \includegraphics[width=0.3\columnwidth]{background/chaotic-phase.pdf}
    \label{figure:rbn-chaotic}
  }

  \caption{
    Trajectories through state-space for RBNs with $N=30, K=[1,2,3]$, visualizing the different phases.
    Time flows downwards the lattice, while RBN states are shown along the X-axis.
    with the network states plotted horizontally, and time flowing downwards.
    Images created with the developed RBN-simulator.
  }
  \label{figure:rbn-phases}
\end{figure}

In general, RBNs in the critical phase are the most interesting.
These are seemingly able to support information transmission, storage and modification,
all capacities required for computation \cite{langton3computation}.
Critical systems are found on the edge of chaos,
on the phase transition between ordered and chaotic networks \cite{gershenson2004introduction}.
For RBNs with $\langle p \rangle = 0.5$,
critical dynamics are usually found at $\langle K \rangle = 2$ \cite{gershenson2004introduction},
although one could still find networks with such dynamics for different values of $\langle K \rangle$.

A thorough introduction to the field of RBNs is available in \cite{gershenson2004introduction}.

\section{RBN Reservoir systems}
\label{subsection:rbn-reservoir-systems}

How does one adapt a RBN for use as a reservoir in a RBN-RC device?
RBNs aren't usually designed to take external input.
We do however, have the concept of perturbation,
the external flipping of bits in the network's state,
transition tables or edges.
This can be utilized to create RBNs that take input,
by continiously perturbing the RBN nodes by the bits of the input sequence.

Questions that follow are how many bits should the network consume at a time,
how many of the network nodes should be perturbed by the input at each timestep,
and what dynamics must such a reservoir have to allow for the computation of interesting problems?

\subsection{A working system}

In \cite{rbn-reservoir} the authors create and analyze functioning RBN-RC systems.
\todo{Shortly mention my pre-thesis project here perhaps?}
These RBN-RC systems have heterogenous connectivity,
consume one bit of input at each timestep ($I=1$),
perturbing $IC$ of the $N$ nodes in the process.
The readout layer can be any node performing some kind of regression of the reservoir state against expected outout for the current task, e.g. linear regression.
Such a setup is shown in Figure \ref{figure:rbn-reservoir}.

\begin{figure}
  \centering
  \includegraphics[width=\columnwidth]{background/RBN-Reservoir.pdf}
  \caption{
    RBN-Reservoir system with $I=1, IC=2, K=2, N=6$ with the entire reservoir sate used for regression.
    The reservoir transforms the problem from a temporal one to a multidimentional spatial one.
    The readout layer the performs some kind of learning on the reservoir states against the expected output for the current task.}
  \label{figure:rbn-reservoir}
\end{figure}

\subsection{Tasks}
\label{section:tasks}

To measure the real-life performance and accuracy of the RBN Reservoir systems,
two tasks were introduced: Temporal Density and Temporal Parity \cite{rbn-reservoir}.
Both require the reservoir to be able to retain information for a sliding window of size $ n $,
offset by some value $ t $, back through the input stream.
The Temporal Parity task requires us to determine if there were an odd number of ones in the sliding window,
the Temporal Density task to determine whether there were a majority of ones.
The Former is visualized in figure \ref{figure:temporal-parity}.

Both tasks will be used to benchmark the reservoirs created later in this paper.

\begin{figure}
  \subfloat[Input]{
    \includegraphics[width=\columnwidth]{background/temporal_parity-10-200-3-input.pdf}
  }

  \subfloat[Correct output]{
    \includegraphics[width=\columnwidth]{background/temporal_parity-10-200-3-output.pdf}
  }

  \caption{
    The first 30 elements of a Temporal Parity task with $[n=3, t=0]$.
    A one is visualized as white, while a zero is black.
    We see that correct output at time $i$ is equal to there being an odd number of $1$s in inputs $[i, i-1, i-2]$
  }
  \label{figure:temporal-parity}
\end{figure}

\subsection{Computational capability}
\label{section:computational-capability}
For an RBN-reservoir to perform well at computational tasks,
it must be able to both forget past perturbations and keep two input streams that have begun converging separated \cite{bertschinger2004real}.

These two properties are coined \textit{fading memory} and \textit{separation property},
and can be measured \cite{rbn-reservoir} as follows.

Create two equal input streams \#1 and \#2 of length $T$.
If measuring \textit{fading memory}, flip the first bit in stream \#2.
If measuring \textit{separation property}, flip all bits up to bit $T-t$ in stream \#2
($t$ being the required depth of separation).
For both input streams, reset reservoir state, perturb the reservoir with the input stream,
and store the final state.
The score of the measure is then defined as the normalized hamming distance between the resulting states.
The computational capability $\Delta$ of an RBN-reservoir is then defined as
\begin{equation}
  \Delta_{Tt} = separation\_property_{Tt} - fading\_memory_{T}
\label{formula:accuracy}
\end{equation}
Analyzing different RBN-reservoirs with this metric \cite{rbn-reservoir},
a high $\Delta$ is found to correlate with critical connectivity ($\langle K \rangle = 2$).
For all RBN-reservoirs, $\Delta$ drops when increasing the required separation $t$,
and is maximized when one doesn't have to remember anything at all ($t=0$).

\subsection{Optimal perturbance}
\label{section:optimal-perturbance}
It is found that the optimal amount of reservoir perturbance,
adjustable by the number of connections between the input layer and the reservoir,
depends on both the task size, how many steps in time are required to be remembered,
and the dynamics of the reservoir.
\textit{Chaotic reservoirs} require few input connections to be able to properly spread information,
but perform poorly on larger tasks due to past perturbations still floating around the reservoir.
\textit{Ordered reservoirs} quickly forget past perturbations, allowing some success for larger tasks,
but their inability to remember past perturbations renders them useless for many tasks.
\textit{Critical reservoirs} require connectivity somewhere in the middle.
Able to forget as well as remember, they perform accurately independent of task size.

\section{Evo Materio, water bucket}
\todo{Gotta write about Evo Materio. I'm a bit unsure where to put this section in the background chapter,
and what angle it should have-if any- that's different from just showing some physical reservoir systems}

\section{RBN Reservoir Computing systems in the authors pre-thesis project}
\label{section:pre-thesis-project}

In his pre-thesis paper, the author investigated the dynamics, performance, and viability of RBNs used for Reservoir Computing (RRC).
A functioning RBN Reservoir Computing system was implemented,
and its results validated against and found in accordance with those from \cite{rbn-reservoir}.
This self-written framework serves as the basis for the simulations and analysis perfored in this thesis as well.

A positive correlation between the computational capability (section \ref{section:computational-capability}) of a reservoir and its actual performance is found.
The optimal connectivity for homogenous reservoirs is found to be $K=3$ as opposed to $\langle K \rangle = 2$ for heterogenous reservoirs \cite{rbn-reservoir}.
Finally, the required input connectivity is found to rise with the presence of chaotic dynamics in the reservoir.
The figures backing this conclusion are shown in \ref{figure:results:temporal-parity-3} and \ref{figure:results:temporal-parity-5}.

Finally, a one-to-many mapping between the readout layer in an already-trained RRC system and different RBN reservoirs was found,
with there being a seemingly large set of interchangeable reservoirs for each readout layer.
This makes the potential use of a smaller generative genome for evolving RRC systems interesting.
Even though it hits fewer points in the RBN fitness landscape than the fixed genome used in this paper,
a large amount of these points are still usable for each instance of a working readout layer.

\begin{table}[ht]
    \centering
    \caption{Task parameters for the pre-thesis project.}
    \label{table:pre-thesis-tasks}
    \begin{tabular}{ll}
        \hline
        \textbf{Parameter} & \textbf{Configuration} \\
        \hline
        \hline
        Task type               & Temporal Parity \\
        Training dataset length & 4 000                       \\
        Test dataset length     & 200                         \\
        $N$ (window size)       & 3 and 5                     \\
        $t$ (offset)            & 0 \\
        \hline
    \end{tabular}
\end{table}

\begin{figure*}[!t]
    \centering
    \caption{
        Plots for Temporal Parity with $N=3$ (parameters in table \ref{section:tasks}).
        Figures \ref{fig:res:d-100-3-1}--\ref{fig:res:d-100-3-3} plot the accuracies of the sampled RBNs against their input connectivity,
        for K=1–3 respectively.
        Figures \ref{fig:res:c-100-3-1}--\ref{fig:res:c-100-3-3} plot the accuracy of the previous figures against their Computational Capability ($T=100, t=3$).
    }
    \label{figure:results:temporal-parity-3}
    \resizebox{\textwidth}{!}{
        \subfloat[K=1]{
            \label{fig:res:d-100-3-1}
            \input{background/pre-project/distribution-100-3-1.tex}
        }
        \subfloat[K=1]{
            \label{fig:res:c-100-3-1}
            \myscatterplot{background/pre-project/computational-power-100-3-1.dat}
        }
    }

    \resizebox{\textwidth}{!}{
        \subfloat[K=2]{
            \label{fig:res:d-100-3-2}
            \input{background/pre-project/distribution-100-3-2.tex}
        }
        \subfloat[K=2]{
            \label{fig:res:c-100-3-2}
            \myscatterplot{background/pre-project/computational-power-100-3-2.dat}
        }
    }

    \resizebox{\textwidth}{!}{
        \subfloat[K=3]{
            \label{fig:res:d-100-3-3}
            \input{background/pre-project/distribution-100-3-3.tex}
        }
        \subfloat[K=3]{
            \label{fig:res:c-100-3-3}
            \myscatterplot{background/pre-project/computational-power-100-3-3.dat}
        }
    }
\end{figure*}

\begin{figure*}[!t]
    \centering
    \caption{
        Plots for Temporal Parity with $N=5$ (parameters in table \ref{section:tasks}).
        Figures \ref{fig:res:d-100-5-1}--\ref{fig:res:d-100-5-3} plot the accuracies of the sampled RBNs against their input connectivity,
        for K=1–3 respectively.
        Figures \ref{fig:res:c-100-5-1}--\ref{fig:res:c-100-5-3} plot the accuracy of the previous figures against their Computational Capability ($T=100, t=3$).
    }
    \label{figure:results:temporal-parity-5}
    \resizebox{\textwidth}{!}{
        \subfloat[K=1]{
            \label{fig:res:d-100-5-1}
            \input{background/pre-project/distribution-100-5-1.tex}
        }
        \subfloat[K=1]{
            \label{fig:res:c-100-5-1}
            \myscatterplot{background/pre-project/computational-power-100-5-1.dat}
        }
    }

    \resizebox{\textwidth}{!}{
        \subfloat[K=2]{
            \label{fig:res:d-100-5-2}
            \input{background/pre-project/distribution-100-5-2.tex}
        }
        \subfloat[K=2]{
            \label{fig:res:c-100-5-2}
            \myscatterplot{background/pre-project/computational-power-100-5-2.dat}
        }
    }

    \resizebox{\textwidth}{!}{
        \subfloat[K=3]{
            \label{fig:res:d-100-5-3}
            \input{background/pre-project/distribution-100-5-3.tex}
        }
        \subfloat[K=3]{
            \label{fig:res:c-100-5-3}
            \myscatterplot{background/pre-project/computational-power-100-5-3.dat}
        }
    }
\end{figure*}

\cleardoublepage
