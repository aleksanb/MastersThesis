\chapter{Background}
\label{chapter:background}

\section{A Brief Introduction to Reservoir Computing}
\label{section:reservoir-computing-introduction}

Recurrent Neural Networks (RNNs), as opposed to feed-forward neural networks,
are notoriously time consuming and difficult to train \cite{Schrauwen2007}.
This is due to feedback from the recurrent connections during the training process,
allowing small topology changes to drastically change a network's position in the fitness landscape.

It was therefore proposed in both \cite{jaeger2002adaptive} as Echo State Networks (ESNs)
and \cite{natschlager2002liquid} as Liquid State Machines (LSMs) to separate the RNN into two parts,
the untrained recurrent reservoir, and the trained readout layer.
Both of these methods have been unified into the field of Reservoir Computing,
now focusing on the separate training and evolution of the recurrent and readout parts \cite{lukovsevivcius2012reservoir}.
Exiting applications of Reservoir Computing include speech and handwriting recognition,
as well as controlling robotics \cite{lukovsevivcius2012reservoir}.

\begin{figure}[ht]
    \caption[Schematic of a general reservoir computing system]{
        Schematic of a general Reservoir Computing system containing adjustable biases, feedback loops, reservoir and readout layers,
        which are described in section \ref{section:reservoir-computing-introduction}.
        Inspired by figures from \cite{Schrauwen2007} and \cite{Jaeger:2007}.
    }
    \label{fig:rc-schema}
    \includegraphics[width=\columnwidth]{background/reservoir_computing_schema.pdf}
\end{figure}

A generalized structure of a Reservoir Computing system is shown in figure \ref{fig:rc-schema}.
The main components are the reservoir and the readout layer.
At each time step in the core model,
the reservoir receives the current input signal as well as its previous state.
The reservoir transforms the input, and passes it on to the readout layer.
The readout layer frequently receives the input signal as well.
The internal weights of the reservoir are usually randomized and left untrained,
with the weights of the readout layer being adjusted by some learning algorithm.
This can be linear or ridge regression for offline learning,
or recursive least squares for online learning \cite{Schrauwen2007}.

There are many extensions to the base model.
The reservoir and output layers can receive a constant bias:
The readout's bias is used for regularizing the reservoir state in case the problem is ill-posed.
This isn't needed when using a model like ridge regression, which performs regularization internally \cite{Schrauwen2007}.
The reservoir's bias is used for stabilizing models which feeds the output layer back into the reservoir,
which may be needed if the problem entails producing oscillating output \cite{Jaeger:2007}.
Teacher forcing,
that is forcing the states of the readout layer to those expected by the trainer for the first n time steps,
usually speed up the convergence of the learning method used,
and may in some cases be required to at all achieve stability \cite{jaeger2002tutorial}.

For a deeper dive into Reservoir Computing,
consult papers \cite{Schrauwen2007}, \cite{lukovsevivcius2012reservoir}, and \cite{Jaeger:2007}.

\section{Alternatives to Traditional Reservoirs}

Any complex dynamical system can in principle be used in a reservoir computing setting.
What properties must these reservoirs have to be able to solve problems?

Complex networks similar to the sparsely connected ones used in ESN and LSM systems include Cellular Automata \cite{wolfram2002new} and Random Boolean Networks \cite{kauffman1969metabolic}.
Cellular Automata are regular grids of cells containing some state,
each cell connected to its neighbors in the grid.
Cells then update in lockstep according to some shared transition table,
creating a new generation.
RBNs can be seen upon as an abstraction over CAs again,
allowing for nonlocal neighbors and variable updating rules.
They will be introduced in depth in section \ref{section:rbns}.

Both CAs and RBNs have successfully been used in Reservoir Computing systems \cite{yilmaz2014reservoir} \cite{rbn-reservoir}.
Both models are simple, and can be implemented in software,
hardware such as FPGAs, and in materio \cite{miller2002evolution} \cite{farstad2015evolving}.

This computational paradigm is known as Cellular Computing,
and provides a potentially powerful alternative to classical computers,
leveraging extreme parallelism, simple components and local state \cite{sipper1999emergence}.

\section{A Brief Introduction to Boolean Networks}
\label{section:rbns}

Random Boolean Networks, also known as Kauffman networks,
were originally developed as a model of gene regulatory networks \cite{kauffman1969metabolic},
the complex system that regulates how genes in multicellular organisms interact with each other.
The model requires no assumptions about the inner workings of the actual nodes,
which allows it to model phenomena where the exact internal workings of the system may be unknown.

The simplification of a system to a Boolean model doesn't pose a problem,
as any multivalued network can be transformed to a corresponding binary one.

A RBN is usually described by its number of nodes $N$ and the in-degree $K$ of the nodes,
that is, how many nodes each node depends on (also known as its ancestors).
RBNs can have both homogeneous and heterogeneous in-degrees.
In heterogeneous networks, one usually describes the average connectivity $\langle K \rangle$ instead.

Each node can have a state of zero or one.
The next state of the node is solely determined by the current combination of states of its ancestors.
Each combination leads to a new state of zero or one,
with the probability given by a binomial distribution usually having $\langle P \rangle = 0.5$.
Figure \ref{figure:sample-homogenous-rbn} visualizes a homogeneous RBN with $N=3, K=2, P=0.5$.

\begin{figure}
  \centering
  \subfloat[RBN topology]{
    \begin{tikzpicture}[node distance = 5em]
      \node[vertex] (a) {a};
      \node[vertex] (b) [below left of=a] {b};
      \node[vertex] (c) [below right of=a] {c};

      \draw[edge] (a) to[bend right] (b);
      \draw[edge] (a) to (c);

      \draw[edge] (b) to (a);
      \draw[edge] (b) to[bend right] (c);

      \draw[edge] (c) to[bend right] (a);
      \draw[edge] (c) to (b);
    \end{tikzpicture}
  }
  \subfloat[Transition table for node a]{
    \begin{tabular}[b]{ c c | c}
      \multicolumn{2}{c}{Ancestor states} & New state \\
      \hline
      0 & 0 & 0 \\
      0 & 1 & 1 \\
      1 & 0 & 0 \\
      1 & 1 & 1 \\
    \end{tabular}
  }
  \caption[An example homogeneous RBN]{An example homogeneous RBN with $N=3, K=2, P=0.5$.}
  \label{figure:sample-homogenous-rbn}
\end{figure}

In the simplest RBN updating scheme, all nodes update in lockstep.
This is known as the Classical RBN updating scheme (CRBN).
The states of the RBN at the next time step $t+1$ therefore only depend on the states at the previous time step $t$.
As the number of RBN states is finite ($2^{n\_nodes}$),
the system will eventually revisit a previously visited state.
This set of repeating states is known as an \emph{attractor},
and a deterministic system cannot escape from it.
If the attractor consists of a single state it is known as a point attractor,
otherwise a cycle attractor.
The set of states leading towards an attractor is known as its \emph{basin of attraction}.
A cycle attractor can be observed in figure \ref{figure:rbn-critical},
while a point attractor is observed in figure \ref{figure:rbn-ordered}.

A criticism of the classical model is that gene regulation networks are updating continuously,
as opposed to in lockstep \cite{gershenson2004introduction}.
There are therefore a number of alternate updating schemes which can be categorized by whether they are deterministic or nondeterministic, as well as synchronous and asynchronous.

The dynamics of an RBN can be categorized as being in either the ordered, critical, or chaotic phase.
These phases can be identified by how large a part of the network state is able to change over time,
whether similar states tend to converge or diverge over time,
and the networks resistance to perturbations (outside changes to the network).

One way to obtain these phases analytically is by comparing the resulting states of two identical RBNs where one is subject to some perturbation \cite{gershenson2004introduction}.
For visual identification, we plot the states of the RBN in a square lattice,
with the network states plotted horizontally, and time flowing downwards.
A node is drawn as white if its state is one, black otherwise.
The phases are visualized in figure \ref{figure:rbn-phases}.

\begin{figure}
  \subfloat[Ordered phase, K=1]{
    \includegraphics[width=0.3\columnwidth]{background/ordered-phase.pdf}
    \label{figure:rbn-ordered}
  }
  \subfloat[Critical phase, K=2]{
    \includegraphics[width=0.3\columnwidth]{background/critical-phase.pdf}
    \label{figure:rbn-critical}
  }
  \subfloat[Chaotic phase, K=3]{
    \includegraphics[width=0.3\columnwidth]{background/chaotic-phase.pdf}
    \label{figure:rbn-chaotic}
  }
  \caption[Trajectories through state space for RBNs with different connectivities] {
    Trajectories through state-space for RBNs with $N=30, K=[1,2,3]$, visualizing the different phases.
    Time flows downwards the lattice, while RBN states are shown along the X-axis.
    with the network states plotted horizontally, and time flowing downwards.
    Images created with the developed RBN-simulator.
  }
  \label{figure:rbn-phases}
\end{figure}

In general, RBNs in the critical phase are the most interesting.
These are seemingly able to support information transmission, storage and modification,
all capacities required for computation \cite{langton3computation}.
Critical systems are found on the edge of chaos,
on the phase transition between ordered and chaotic networks \cite{gershenson2004introduction}.
For RBNs with $\langle p \rangle = 0.5$,
critical dynamics are usually found at $\langle K \rangle = 2$ \cite{gershenson2004introduction},
although one could still find networks with such dynamics for different values of $\langle K \rangle$.

A thorough introduction to the field of RBNs is available in \cite{gershenson2004introduction}.

\section{RBN Reservoir systems}
\label{subsection:rbn-reservoir-systems}

How does one adapt a RBN for use as a reservoir in a RBN-RC device?
RBNs aren't usually designed to take external input.
We do however, have the concept of perturbation,
the external flipping of bits in the network's state,
transition tables or edges.
This can be utilized to create RBNs that take input,
by continuously perturbing the RBN nodes by the bits of the input sequence.

Questions that follow are how many bits should the network consume at a time,
how many of the network nodes should be perturbed by the input at each time step,
and what dynamics must such a reservoir have to allow for the computation of interesting problems?

\subsection{A working system}

In \cite{rbn-reservoir} functioning RBN-RC systems are created and analyzed.
These RBN-RC systems have heterogeneous connectivity,
consume one bit of input at each time step ($I=1$),
perturbing $IC$ of the $N$ nodes in the process.
The readout layer can be any node performing some kind of regression of the reservoir state against expected output for the current task, e.g. linear regression.
Such a setup is shown in figure \ref{figure:rbn-reservoir}.

\begin{figure}
  \centering
  \includegraphics[width=\columnwidth]{background/RBN-Reservoir.pdf}
    \caption[An example RBN reservoir computing system]{
    RBN reservoir computing system with $I=1, IC=2, K=2, N=6$ with the entire reservoir sate used for regression.
    The reservoir transforms the problem from a temporal one to a multidimensional spatial one.
    The readout layer the performs some kind of learning on the reservoir states against the expected output for the current task.}
  \label{figure:rbn-reservoir}
\end{figure}

\subsection{Tasks}
\label{section:tasks}

To measure the real-life performance and accuracy of the RBN Reservoir systems,
two tasks are introduced: Temporal Density and Temporal Parity \cite{rbn-reservoir}.
Both require the reservoir to be able to retain information for a sliding window of size $ n $,
offset by some value $ t $, back through the input stream.
The Temporal Parity task requires us to determine if there were an odd number of ones in the sliding window,
the Temporal Density task to determine whether there were a majority of ones.
The Former is visualized in figure \ref{figure:temporal-parity}.

Both tasks will be used to benchmark the reservoirs created later in this paper.

\begin{figure}
  \subfloat[Input]{
    \includegraphics[width=\columnwidth]{background/temporal_parity-10-200-3-input.pdf}
  }

  \subfloat[Correct output]{
    \includegraphics[width=\columnwidth]{background/temporal_parity-10-200-3-output.pdf}
  }

  \caption[The first elements of the temporal parity task]{
    The first 30 elements of a Temporal Parity task with $[n=3, t=0]$.
    A one is visualized as white, while a zero is black.
    We see that correct output at time $i$ is equal to there being an odd number of $1$s in inputs $[i, i-1, i-2]$
  }
  \label{figure:temporal-parity}
\end{figure}

\subsection{Computational capability}
\label{section:computational-capability}
For an RBN-reservoir to perform well at computational tasks,
it must be able to both forget past perturbations and keep two input streams that have begun converging separated \cite{bertschinger2004real}.

These two properties are coined \textit{fading memory} and \textit{separation property},
and can be measured \cite{rbn-reservoir} as follows.

Create two equal input streams \#1 and \#2 of length $T$.
If measuring \textit{fading memory}, flip the first bit in stream \#2.
If measuring \textit{separation property}, flip all bits up to bit $T-t$ in stream \#2
($t$ being the required depth of separation).
For both input streams, reset reservoir state, perturb the reservoir with the input stream,
and store the final state.
The score of the measure is then defined as the normalized hamming distance between the resulting states.
The computational capability $\Delta$ of an RBN-reservoir is then defined as
\begin{equation}
  \Delta_{Tt} = separation\_property_{Tt} - fading\_memory_{T}
\label{formula:accuracy}
\end{equation}
Analyzing different RBN-reservoirs with this metric \cite{rbn-reservoir},
a high $\Delta$ is found to correlate with critical connectivity ($\langle K \rangle = 2$).
For all RBN-reservoirs, $\Delta$ drops when increasing the required separation $t$,
and is maximized when one doesn't have to remember anything at all ($t=0$).

\subsection{Optimal perturbation}
\label{section:optimal-perturbance}
It is found that the optimal amount of reservoir perturbation,
adjustable by the number of connections between the input layer and the reservoir,
depends on both the task size, how many steps in time are required to be remembered,
and the dynamics of the reservoir.
\textit{Chaotic reservoirs} require few input connections to be able to properly spread information,
but perform poorly on larger tasks due to past perturbations still floating around the reservoir.
\textit{Ordered reservoirs} quickly forget past perturbations, allowing some success for larger tasks,
but their inability to remember past perturbations renders them useless for many tasks.
\textit{Critical reservoirs} require connectivity somewhere in the middle.
Able to forget as well as remember, they perform accurately independent of task size.


\section{Evolution in Materio and physical reservoirs}
\label{section:evo-materio-physical-reservoirs}

The field of evolution-in-materio \cite{miller2002evolution} concerns the harnessing of unconventional and complex physical materials,
using them to perform computation.
The process is usually aided by Artificial Evolution for finding reasonable input-substrate-output mappings.
The difference from classical computation is that the computation is moved from an abstraction over the hardware (e.g. a modern CPU), and into the material itself.
This material can be anything from grown dendritic iron wires which discriminate sound \cite{pask1958physical} to carbon nanotubes used for evolving CA rules \cite{farstad2015evolving}.
Success within this field may result in more efficient use of materials,
and higher computational densities.

In \cite{demarse2005adaptive},
living rat neurons are put in a microelectrode array (MEA) and successfully used to control an airplane in a flight simulator.
Two of sixty electrodes were chosen to control the pitch and roll of the airplane.
The internal 'weights' of the rat brain were trained by administering a series of electrical impulses to the same electrodes which then increase or decrease the action potential of the region,
correcting behavior.
The internal state of the system is unknown,
with the MEA only exposing a limited resolution subset of the entire system state
(around 60 electrodes).

In \cite{farstad2015evolving}, the computational power of carbon nanotubes,
instrumented in a microelectrode array,
is used to evolve the state transition functions of all elementary Cellular Automata.
Artificial Evolution is used to find the correct translation of input/output values to their respective electrodes.
The internal state of the carbon nanotubes are again completely opaque to the higher computational layers.
In addition, the number of electrodes present a limited resolution view of the substrate.

In the ever-so-cited 'Pattern recognition in a bucket' paper \cite{fernando2003pattern},
the Liquid State Machine metaphor is taken literally and a bucket of water is used as a physical reservoir.
The bucket of water receives input via rotors mounted on the edge of the bucket.
Input is then transformed from a temporal into a spatial, with a great deal of nonlinearity introduced by the bucket.
Finally, the reservoir state is read out by taking pictures of the wave-patterns,
using a simple perceptron to classify the resulting state.
This setup correctly separates the xor problem as well as being able to classify the words 'one' and 'zero' when yelled into the bucket.

What these approaches have in common is that physical substrates,
with varying degrees of perturbation ability and insight into the internal state,
are used for computation.
Their couplings to theoretical frameworks such as Reservoir Computing motivate the exploration of the effects of limited perturbation and readout possibilities on reservoir performance.

\section{Validating RBNs for Reservoir Computing}
\label{section:pre-thesis-project}

In the authors pre-thesis paper, the dynamics, performance, and viability of RBNs used for Reservoir Computing (RRC) was investigated.
A functioning RBN Reservoir Computing system was implemented,
and its results validated against and found in accordance with those from \cite{rbn-reservoir}.
The simulation software from the pre-thesis work has been greatly extended for use in this thesis.

A positive correlation between the computational capability (section \ref{section:computational-capability}) of a reservoir and its actual performance is found.
The optimal connectivity for homogeneous reservoirs is found to be $K=3$ as opposed to $\langle K \rangle = 2$ for heterogeneous reservoirs \cite{rbn-reservoir}.
Finally, the required input connectivity is found to rise with the presence of chaotic dynamics in the reservoir.
The figures backing this conclusion are shown in \ref{figure:results:temporal-parity-3} and \ref{figure:results:temporal-parity-5}.

Finally, a one-to-many mapping between the readout layer of an already-trained RRC system and different RBN reservoirs is found,
with there being a seemingly large set of interchangeable reservoirs for each readout layer.
This neutrality in the space of possible RBNs make the potential use of a smaller generative genome for evolving RRC systems interesting.
Even though such a genome will hit fewer points in the RBN fitness landscape than a fixed genome,
a large amount of these points will still be usable for each instance of a working readout layer.
There doesn't seem to be a tight correlation between the properties of the RBN from the original RRC system,
and the equivalent RBNs found through artificial evolution.
In fact, the connectivity distributions of figure \ref{figure:evolved-connectivity} and the computational capabilities of figure \ref{figure:evolved-ccs} seem much more representative of the general RBN population, as shown in figure \ref{fig:res:d-100-3-2}.
This indicates that while there are many compatible reservoirs for a given readout layer,
the distribution of these compatible reservoirs are likely the same as the distribution of reservoirs with the same connectivity in general.

The parameters for the Temporal Parity task used for all experiments is shown in table \ref{table:pre-thesis-tasks} of appendix \ref{app:pre-thesis-bonus-content}.

\begin{figure*}[ht]
    \centering
    \caption[Computational capability and input connectivity for evolved RBNs]{
        Computational Capability and Input Connectivity distributions for RBNs evolved from a fixed readout layer.
        The Y-axis shows the CC and IC values for the readout layers original RBN.
    }
    \resizebox{\textwidth}{!}{
        \subfloat[Evolved RBN input connectivity]{
            \input{background/pre-project/evolved-cc.tex}
            \label{figure:evolved-ccs}
        }

        \subfloat[Evolved RBN Computational Capability]{
            \input{background/pre-project/evolved-ic.tex}
            \label{figure:evolved-connectivity}
        }
    }
\end{figure*}

\begin{figure*}
    \centering
    \caption[Temporal parity 3: Accuracy and computational capability plots]{
        Plots for Temporal Parity with $N=3$.
        Figures \ref{fig:res:d-100-3-1}--\ref{fig:res:d-100-3-3} plot the accuracies of the sampled RBNs against their input connectivity,
        for K=1–3 respectively.
        Figures \ref{fig:res:c-100-3-1}--\ref{fig:res:c-100-3-3} plot the accuracy of the previous figures against their Computational Capability ($T=100, t=3$).
    }
    \label{figure:results:temporal-parity-3}
    \resizebox{\textwidth}{!}{
        \subfloat[K=1]{
            \label{fig:res:d-100-3-1}
            \input{background/pre-project/distribution-100-3-1.tex}
        }
        \subfloat[K=1]{
            \label{fig:res:c-100-3-1}
            \myscatterplot{background/pre-project/computational-power-100-3-1.dat}
        }
    }

    \resizebox{\textwidth}{!}{
        \subfloat[K=2]{
            \label{fig:res:d-100-3-2}
            \input{background/pre-project/distribution-100-3-2.tex}
        }
        \subfloat[K=2]{
            \label{fig:res:c-100-3-2}
            \myscatterplot{background/pre-project/computational-power-100-3-2.dat}
        }
    }

    \resizebox{\textwidth}{!}{
        \subfloat[K=3]{
            \label{fig:res:d-100-3-3}
            \input{background/pre-project/distribution-100-3-3.tex}
        }
        \subfloat[K=3]{
            \label{fig:res:c-100-3-3}
            \myscatterplot{background/pre-project/computational-power-100-3-3.dat}
        }
    }
\end{figure*}

\begin{figure*}
    \centering
    \caption[Temporal parity 5: Accuracy and computational capability plots]{
        Plots for Temporal Parity with $N=5$.
        Figures \ref{fig:res:d-100-5-1}--\ref{fig:res:d-100-5-3} plot the accuracies of the sampled RBNs against their input connectivity,
        for K=1–3 respectively.
        Figures \ref{fig:res:c-100-5-1}--\ref{fig:res:c-100-5-3} plot the accuracy of the previous figures against their Computational Capability ($T=100, t=3$).
    }
    \label{figure:results:temporal-parity-5}
    \resizebox{\textwidth}{!}{
        \subfloat[K=1]{
            \label{fig:res:d-100-5-1}
            \input{background/pre-project/distribution-100-5-1.tex}
        }
        \subfloat[K=1]{
            \label{fig:res:c-100-5-1}
            \myscatterplot{background/pre-project/computational-power-100-5-1.dat}
        }
    }

    \resizebox{\textwidth}{!}{
        \subfloat[K=2]{
            \label{fig:res:d-100-5-2}
            \input{background/pre-project/distribution-100-5-2.tex}
        }
        \subfloat[K=2]{
            \label{fig:res:c-100-5-2}
            \myscatterplot{background/pre-project/computational-power-100-5-2.dat}
        }
    }

    \resizebox{\textwidth}{!}{
        \subfloat[K=3]{
            \label{fig:res:d-100-5-3}
            \input{background/pre-project/distribution-100-5-3.tex}
        }
        \subfloat[K=3]{
            \label{fig:res:c-100-5-3}
            \myscatterplot{background/pre-project/computational-power-100-5-3.dat}
        }
    }
\end{figure*}
