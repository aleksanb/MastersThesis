\section{How small do RBN-Reservoirs have to be?}

So the question is: how small a bucket of RBN is actually needed to solve the timedelay-3 task?
As shown in \cite{MyPreviousPaper}, reservoirs of size 100, connectivity 2 and 3 are easily able to solve the temporal parity task for T=3 and T=5.

Then again 100 nodes are a lot of computational power,
so to find the correct computer for the problem we create reservoirs of size 10->100,
with input connectivities between 0 and N.
K=3 is chosen, as it shows the most promise from the previous paper,
as well as being more useful due to the homogenous degree of the network (to simulate $\langle K \rangle = 2 $),
and reducing the search space.

\section{How many readouts are actually needed?}

When working on a real reservoir material, it might be difficult or infeasible to instrument the entire reservoir, forcing us to sample a subset of the network.
One might also want to sample a subset if the readout equipment is expensive or space-consuming.
Finally, the reservoir might be dynamically growing and changing over time,
and not having to re-instrument the reservoir with new electrodes or whatnot is useful knowledge to have.

To test the requred amount of output connectivity,
we create reservoirs for all Ns, using the optimal input connectivity as found in the previous section (N/2, to reduce the search space), and iterate over output connectivity in step sizes of (5 or 10).

The best result would be that even though temporal5 requires a network of size N=100,
one still doesn't require more than 10 output connections.
This would also show that the dynamimcs of the network sufficciently impact the chosen readout oints.

\section{Re-using readout layer across growing reservoirs}

Now that we find that we don't have to add more electrodes as our bucket of goo grows,
how often do we have to re-train our readout layer?

We test this by choosing N size-10 readout layers, and creating new and larger reservoirs.
We do a rough scan of the number of working readout layers, and find that quite a few of those can re-use the previous one.
Then again, the neutrality in the reservoir-landscape that allow for reuse of readout layers is somewhat useless, as one would rather re-train the readout layer after the reservoir has changed sufficciently (instead of hoping exactly this linear-regression node just happens to work for a significantly changed reservoir).
